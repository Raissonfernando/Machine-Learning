{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c5fd27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5fc671",
   "metadata": {},
   "source": [
    "# Instalando Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38ef103a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sparkmagic\n",
      "  Downloading sparkmagic-0.20.0.tar.gz (44 kB)\n",
      "Collecting hdijupyterutils>=0.6\n",
      "  Downloading hdijupyterutils-0.20.0.tar.gz (5.0 kB)\n",
      "Collecting autovizwidget>=0.6\n",
      "  Downloading autovizwidget-0.20.0.tar.gz (8.8 kB)\n",
      "Requirement already satisfied: ipython>=5 in c:\\programdata\\anaconda3\\lib\\site-packages (from sparkmagic) (7.22.0)\n",
      "Requirement already satisfied: nose in c:\\programdata\\anaconda3\\lib\\site-packages (from sparkmagic) (1.3.7)\n",
      "Requirement already satisfied: mock in c:\\programdata\\anaconda3\\lib\\site-packages (from sparkmagic) (4.0.3)\n",
      "Requirement already satisfied: pandas>=0.17.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from sparkmagic) (1.2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from sparkmagic) (1.20.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from sparkmagic) (2.25.1)\n",
      "Requirement already satisfied: ipykernel>=4.2.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from sparkmagic) (5.3.4)\n",
      "Requirement already satisfied: ipywidgets>5.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sparkmagic) (7.6.3)\n",
      "\n",
      "Requirement already satisfied: notebook>=4.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from sparkmagic) (6.3.0)\n",
      "Requirement already satisfied: tornado>=4 in c:\\programdata\\anaconda3\\lib\\site-packages (from sparkmagic) (6.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Acesso negado: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\__pycache__\\\\nest_asyncio.cpython-38.pyc'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests_kerberos>=0.8.0\n",
      "  Downloading requests_kerberos-0.14.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting nest_asyncio==1.5.5\n",
      "  Downloading nest_asyncio-1.5.5-py3-none-any.whl (5.2 kB)\n",
      "Collecting plotly>=3\n",
      "  Downloading plotly-5.10.0-py2.py3-none-any.whl (15.2 MB)\n",
      "Requirement already satisfied: jupyter>=1 in c:\\programdata\\anaconda3\\lib\\site-packages (from hdijupyterutils>=0.6->sparkmagic) (1.0.0)\n",
      "Requirement already satisfied: jupyter-client in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel>=4.2.2->sparkmagic) (6.1.12)\n",
      "Requirement already satisfied: traitlets>=4.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel>=4.2.2->sparkmagic) (5.0.5)\n",
      "Requirement already satisfied: pickleshare in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5->sparkmagic) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5->sparkmagic) (0.17.2)\n",
      "Requirement already satisfied: backcall in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5->sparkmagic) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5->sparkmagic) (0.4.4)\n",
      "Requirement already satisfied: decorator in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5->sparkmagic) (5.0.6)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5->sparkmagic) (52.0.0.post20210125)\n",
      "Requirement already satisfied: pygments in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5->sparkmagic) (2.8.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5->sparkmagic) (3.0.17)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipywidgets>5.0.0->sparkmagic) (1.0.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipywidgets>5.0.0->sparkmagic) (5.1.3)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipywidgets>5.0.0->sparkmagic) (3.5.1)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=5->sparkmagic) (0.7.0)\n",
      "Requirement already satisfied: nbconvert in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (6.0.7)\n",
      "Requirement already satisfied: jupyter-console in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (6.4.0)\n",
      "Requirement already satisfied: qtconsole in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (5.0.3)\n",
      "Requirement already satisfied: ipython-genutils in c:\\programdata\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets>5.0.0->sparkmagic) (0.2.0)\n",
      "Requirement already satisfied: jupyter-core in c:\\programdata\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets>5.0.0->sparkmagic) (4.7.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets>5.0.0->sparkmagic) (3.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>5.0.0->sparkmagic) (20.3.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>5.0.0->sparkmagic) (1.15.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>5.0.0->sparkmagic) (0.17.3)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=4.2->sparkmagic) (1.5.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=4.2->sparkmagic) (0.10.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=4.2->sparkmagic) (0.9.4)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=4.2->sparkmagic) (2.11.3)\n",
      "Requirement already satisfied: argon2-cffi in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=4.2->sparkmagic) (20.1.0)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=4.2->sparkmagic) (20.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-client->ipykernel>=4.2.2->sparkmagic) (2.8.1)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-core->nbformat>=4.2.0->ipywidgets>5.0.0->sparkmagic) (227)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.17.1->sparkmagic) (2021.1)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.1.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: wcwidth in c:\\programdata\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5->sparkmagic) (0.2.5)\n",
      "Requirement already satisfied: cryptography>=1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests_kerberos>=0.8.0->sparkmagic) (3.4.7)\n",
      "Collecting pyspnego[kerberos]\n",
      "  Downloading pyspnego-0.6.0-cp38-cp38-win_amd64.whl (238 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from cryptography>=1.3->requests_kerberos>=0.8.0->sparkmagic) (1.14.5)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=1.3->requests_kerberos>=0.8.0->sparkmagic) (2.20)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->sparkmagic) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->sparkmagic) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->sparkmagic) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->sparkmagic) (1.26.4)\n",
      "Requirement already satisfied: pywinpty>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from terminado>=0.8.3->notebook>=4.2->sparkmagic) (0.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->notebook>=4.2->sparkmagic) (1.1.1)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (0.3)\n",
      "Requirement already satisfied: defusedxml in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (0.7.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (0.5.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (0.1.2)\n",
      "Requirement already satisfied: testpath in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (0.4.4)\n",
      "Requirement already satisfied: bleach in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (3.3.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (1.4.3)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (0.8.4)\n",
      "Requirement already satisfied: async-generator in c:\\programdata\\anaconda3\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (1.10)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from bleach->nbconvert->jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (20.9)\n",
      "Requirement already satisfied: webencodings in c:\\programdata\\anaconda3\\lib\\site-packages (from bleach->nbconvert->jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (0.5.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->bleach->nbconvert->jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (2.4.7)\n",
      "Requirement already satisfied: qtpy in c:\\programdata\\anaconda3\\lib\\site-packages (from qtconsole->jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (1.9.0)\n",
      "Building wheels for collected packages: sparkmagic, autovizwidget, hdijupyterutils\n",
      "  Building wheel for sparkmagic (setup.py): started\n",
      "  Building wheel for sparkmagic (setup.py): finished with status 'done'\n",
      "  Created wheel for sparkmagic: filename=sparkmagic-0.20.0-py3-none-any.whl size=66525 sha256=e471807cc88ad635ae232b3b29403b98db6034da778c3f31493394315c4d219c\n",
      "  Stored in directory: c:\\users\\notebook\\appdata\\local\\pip\\cache\\wheels\\42\\58\\b0\\b5e033a82e461f67965b293fb2f665593fd1a7f6b107ad6444\n",
      "  Building wheel for autovizwidget (setup.py): started\n",
      "  Building wheel for autovizwidget (setup.py): finished with status 'done'\n",
      "  Created wheel for autovizwidget: filename=autovizwidget-0.20.0-py3-none-any.whl size=14641 sha256=e72e6af2995db300fd9b7f860236c61df9f1c5186bdc3a04080efd7440407c13\n",
      "  Stored in directory: c:\\users\\notebook\\appdata\\local\\pip\\cache\\wheels\\c0\\c6\\72\\ade57ec305cbd10071a63cb4e2508e14e4cf3b5340c6cfa801\n",
      "  Building wheel for hdijupyterutils (setup.py): started\n",
      "  Building wheel for hdijupyterutils (setup.py): finished with status 'done'\n",
      "  Created wheel for hdijupyterutils: filename=hdijupyterutils-0.20.0-py3-none-any.whl size=7661 sha256=20b4a4ed7b1b2e0c63259db0b2cff042cb3e89bbf8b3f3ec0d480f528eec3184\n",
      "  Stored in directory: c:\\users\\notebook\\appdata\\local\\pip\\cache\\wheels\\00\\75\\a5\\d7d33027c25fbf73c0104a501d9003280241ba1c18b5147dac\n",
      "Successfully built sparkmagic autovizwidget hdijupyterutils\n",
      "Installing collected packages: nest-asyncio, tenacity, pyspnego, plotly, hdijupyterutils, requests-kerberos, autovizwidget, sparkmagic\n",
      "  Attempting uninstall: nest-asyncio\n",
      "    Found existing installation: nest-asyncio 1.5.1\n",
      "    Uninstalling nest-asyncio-1.5.1:\n"
     ]
    }
   ],
   "source": [
    "pip install sparkmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "714c670b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\programdata\\anaconda3\\lib\\site-packages (3.3.0)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: py4j==0.10.9.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d16647",
   "metadata": {},
   "source": [
    "# Importando bibliotecas Spark SQL e Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "720c1533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer, MinMaxScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56771fb",
   "metadata": {},
   "source": [
    "# Carregando o dado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc88c33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|DayofMonth|DayOfWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|        19|        5|     DL|          11433|        13303|      -3|       1|\n",
      "|        19|        5|     DL|          14869|        12478|       0|      -8|\n",
      "|        19|        5|     DL|          14057|        14869|      -4|     -15|\n",
      "|        19|        5|     DL|          15016|        11433|      28|      24|\n",
      "|        19|        5|     DL|          11193|        12892|      -6|     -11|\n",
      "|        19|        5|     DL|          10397|        15016|      -1|     -19|\n",
      "|        19|        5|     DL|          15016|        10397|       0|      -1|\n",
      "|        19|        5|     DL|          10397|        14869|      15|      24|\n",
      "|        19|        5|     DL|          10397|        10423|      33|      34|\n",
      "|        19|        5|     DL|          11278|        10397|     323|     322|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv = spark.read.csv('C:/Users/Notebook/Documents/Notebooks/dados/ml_with_spark/flights.csv', inferSchema=True, header=True)\n",
    "csv.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee4ec72",
   "metadata": {},
   "source": [
    "# Preparando o dado para o modelo de Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cffea6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+-----+\n",
      "|DayofMonth|DayofWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|label|\n",
      "+----------+---------+-------+---------------+-------------+--------+-----+\n",
      "|        19|        5|     DL|          11433|        13303|      -3|    0|\n",
      "|        19|        5|     DL|          14869|        12478|       0|    0|\n",
      "|        19|        5|     DL|          14057|        14869|      -4|    0|\n",
      "|        19|        5|     DL|          15016|        11433|      28|    1|\n",
      "|        19|        5|     DL|          11193|        12892|      -6|    0|\n",
      "|        19|        5|     DL|          10397|        15016|      -1|    0|\n",
      "|        19|        5|     DL|          15016|        10397|       0|    0|\n",
      "|        19|        5|     DL|          10397|        14869|      15|    1|\n",
      "|        19|        5|     DL|          10397|        10423|      33|    1|\n",
      "|        19|        5|     DL|          11278|        10397|     323|    1|\n",
      "|        19|        5|     DL|          14107|        13487|      -7|    0|\n",
      "|        19|        5|     DL|          11433|        11298|      22|    1|\n",
      "|        19|        5|     DL|          11298|        11433|      40|    1|\n",
      "|        19|        5|     DL|          11433|        12892|      -2|    0|\n",
      "|        19|        5|     DL|          10397|        12451|      71|    1|\n",
      "|        19|        5|     DL|          12451|        10397|      75|    1|\n",
      "|        19|        5|     DL|          12953|        10397|      -1|    0|\n",
      "|        19|        5|     DL|          11433|        12953|      -3|    0|\n",
      "+----------+---------+-------+---------------+-------------+--------+-----+\n",
      "only showing top 18 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = csv.select(\"DayofMonth\", \"DayofWeek\", \"Carrier\", \"OriginAirportID\", \"DestAirportID\", \"DepDelay\",\n",
    "                  ((col(\"ArrDelay\") > 15).cast(\"Int\").alias(\"label\")))\n",
    "data.show(18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7415fbf",
   "metadata": {},
   "source": [
    "\n",
    "# Dividindo o dado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52bcee24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Rows. 1891562 Testing Rows: 810656\n"
     ]
    }
   ],
   "source": [
    "splits = data.randomSplit([0.7, 0.3])\n",
    "train = splits[0]\n",
    "test = splits[1].withColumnRenamed(\"label\", \"trueLabel\")\n",
    "train_rows = train.count()\n",
    "test_rows = test.count()\n",
    "print(\"Training Rows.\", train_rows, \"Testing Rows:\", test_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc527584",
   "metadata": {},
   "source": [
    "# Define the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c55dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "strIdx = StringIndexer(inputCol = \"Carrier\", outputCol = \"CarrierIdx\")\n",
    "catVect = VectorAssembler(inputCols = [\"CarrierIdx\", \"DayofMonth\", \"DayofWeek\", \"OriginAirportID\", \"DestAirportID\"],\n",
    "                          outputCol=\"catFeatures\")\n",
    "catIdx = VectorIndexer(inputCol = catVect.getOutputCol(), outputCol = \"idxCatFeatures\")\n",
    "numVect = VectorAssembler(inputCols = [\"DepDelay\"], outputCol = \"normfeatures\")\n",
    "minMax = MinMaxScaler(inputCol = numVect.getOutputCol(), outputCol=\"normFeatures\")\n",
    "featVect = VectorAssembler(inputCols = [\"idxCatFeatures\", \"normFeatures\"], outputCol = \"features\")\n",
    "lr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\",maxIter=10,regParam=0.3)\n",
    "pipeline = Pipeline(stages=[strIdx, catVect, catIdx, numVect, minMax, featVect, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0ae998",
   "metadata": {},
   "source": [
    "# Executando o pipeline para treino do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "71c11ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipLineModel = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c6f7546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+----------+---------+\n",
      "|features                                           |prediction|trueLabel|\n",
      "+---------------------------------------------------+----------+---------+\n",
      "|[10.0,1.0,0.0,10397.0,12191.0,0.03115264797507788] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,10397.0,12264.0,0.04205607476635514] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,10397.0,13851.0,0.03167185877466251] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,10423.0,13487.0,0.027518172377985463]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,10423.0,14869.0,0.05036344755970924] |0.0       |1        |\n",
      "|[10.0,1.0,0.0,10529.0,11193.0,0.037383177570093455]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,10529.0,11433.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,10693.0,10397.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,10693.0,11433.0,0.029595015576323987]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,10693.0,13487.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,10721.0,12478.0,0.04569055036344756] |0.0       |1        |\n",
      "|[10.0,1.0,0.0,10721.0,13931.0,0.028556593977154723]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,10821.0,12478.0,0.027518172377985463]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,10821.0,12478.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,10821.0,14492.0,0.029075804776739357]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,10821.0,14492.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11042.0,12478.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11042.0,13487.0,0.02596053997923157] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11042.0,13487.0,0.028037383177570093]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11057.0,11193.0,0.028037383177570093]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11057.0,12478.0,0.03115264797507788] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11057.0,13244.0,0.03115264797507788] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11066.0,13487.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11066.0,13487.0,0.03115264797507788] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,10529.0,0.03167185877466251] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,10693.0,0.049325025960539975]|0.0       |1        |\n",
      "|[10.0,1.0,0.0,11193.0,10821.0,0.028556593977154723]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,10821.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,11057.0,0.033229491173416406]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,11298.0,0.032191069574247146]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,11433.0,0.029075804776739357]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,11433.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,11618.0,0.06438213914849429] |0.0       |1        |\n",
      "|[10.0,1.0,0.0,11193.0,13198.0,0.0347871235721703]  |0.0       |1        |\n",
      "|[10.0,1.0,0.0,11193.0,13487.0,0.03115264797507788] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,13930.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,14100.0,0.029075804776739357]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,14122.0,0.049325025960539975]|0.0       |1        |\n",
      "|[10.0,1.0,0.0,11193.0,15016.0,0.03115264797507788] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11278.0,11433.0,0.028037383177570093]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11278.0,12478.0,0.035306334371754934]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11278.0,12478.0,0.0628245067497404]  |0.0       |1        |\n",
      "|[10.0,1.0,0.0,11278.0,13244.0,0.04776739356178608] |0.0       |1        |\n",
      "|[10.0,1.0,0.0,11298.0,13487.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11298.0,14869.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,10423.0,0.029595015576323987]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,10693.0,0.03271028037383177] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,10792.0,0.03115264797507788] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,11042.0,0.028556593977154723]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,11042.0,0.03167185877466251] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,11042.0,0.0347871235721703]  |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,12266.0,0.033229491173416406]|0.0       |1        |\n",
      "|[10.0,1.0,0.0,11433.0,12339.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,12339.0,0.03167185877466251] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,12339.0,0.03686396677050883] |0.0       |1        |\n",
      "|[10.0,1.0,0.0,11433.0,12451.0,0.029595015576323987]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,13232.0,0.03115264797507788] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,13232.0,0.03167185877466251] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,13232.0,0.03271028037383177] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,13342.0,0.03167185877466251] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,13342.0,0.0347871235721703]  |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,13931.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,14100.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,14122.0,0.032191069574247146]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,14307.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,14524.0,0.03115264797507788] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,12191.0,10397.0,0.029595015576323987]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12264.0,10397.0,0.029075804776739357]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12264.0,10397.0,0.03686396677050883] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,12264.0,11433.0,0.028556593977154723]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12266.0,13244.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12266.0,13487.0,0.0430944963655244]  |0.0       |0        |\n",
      "|[10.0,1.0,0.0,12339.0,11433.0,0.027518172377985463]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12339.0,13244.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12339.0,13244.0,0.03582554517133956] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,12339.0,13487.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12339.0,14492.0,0.027518172377985463]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12451.0,12478.0,0.0960539979231568]  |1.0       |1        |\n",
      "|[10.0,1.0,0.0,12478.0,10721.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12478.0,10821.0,0.0529595015576324]  |0.0       |1        |\n",
      "|[10.0,1.0,0.0,12478.0,10821.0,0.05555555555555555] |0.0       |1        |\n",
      "|[10.0,1.0,0.0,12478.0,11057.0,0.047248182762201454]|0.0       |1        |\n",
      "|[10.0,1.0,0.0,12478.0,11193.0,0.028037383177570093]|0.0       |1        |\n",
      "|[10.0,1.0,0.0,12478.0,11193.0,0.029075804776739357]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12478.0,11278.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12478.0,11278.0,0.03167185877466251] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,12478.0,11278.0,0.0763239875389408]  |0.0       |1        |\n",
      "|[10.0,1.0,0.0,12478.0,11433.0,0.03686396677050883] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,12478.0,13930.0,0.03271028037383177] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,12478.0,14100.0,0.0597092419522326]  |0.0       |1        |\n",
      "|[10.0,1.0,0.0,12478.0,14122.0,0.0264797507788162]  |0.0       |0        |\n",
      "|[10.0,1.0,0.0,12478.0,14492.0,0.032191069574247146]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12953.0,14730.0,0.03115264797507788] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,13198.0,11193.0,0.028556593977154723]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,13198.0,11433.0,0.029595015576323987]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,13232.0,11433.0,0.029595015576323987]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,13244.0,10721.0,0.029075804776739357]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,13244.0,11278.0,0.048286604361370715]|0.0       |1        |\n",
      "|[10.0,1.0,0.0,13244.0,13495.0,0.032191069574247146]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,13244.0,13851.0,0.029595015576323987]|0.0       |0        |\n",
      "+---------------------------------------------------+----------+---------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = pipLineModel.transform(test)\n",
    "predicted = prediction.select(\"features\", \"prediction\", \"trueLabel\")\n",
    "predicted.show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b528a093",
   "metadata": {},
   "source": [
    "# Compute Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "050b57cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o521.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 1 times, most recent failure: Lost task 0.0 in stage 40.0 (TID 92) (DESKTOP-TGUU6OU executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\spark-3.3.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 540, in main\nRuntimeError: Python in worker has different version 3.9 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\spark-3.3.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 540, in main\nRuntimeError: Python in worker has different version 3.9 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-0165155098bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m  \u001b[1;33m(\u001b[0m\u001b[1;34m\"Recall\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m  (\"F1\", 2*pr*re/(re+pr))],[\"metric\", \"value\"])\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o521.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 1 times, most recent failure: Lost task 0.0 in stage 40.0 (TID 92) (DESKTOP-TGUU6OU executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\spark-3.3.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 540, in main\nRuntimeError: Python in worker has different version 3.9 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\spark-3.3.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 540, in main\nRuntimeError: Python in worker has different version 3.9 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "tp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 1\").count())\n",
    "fp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 0\").count())\n",
    "tn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 0\").count())\n",
    "fn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 1\").count())\n",
    "pr = tp / (tp + fp)\n",
    "re = tp / (tp + fn)\n",
    "metrics = spark.createDataFrame([\n",
    " (\"TP\", tp),\n",
    " (\"FP\", fp),\n",
    " (\"TN\", tn),\n",
    " (\"FN\", fn),\n",
    " (\"Precision\", pr),\n",
    " (\"Recall\", re),\n",
    " (\"F1\", 2*pr*re/(re+pr))],[\"metric\", \"value\"])\n",
    "metrics.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb9438",
   "metadata": {},
   "source": [
    "# Review the Area Under ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "411b2ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUR =  0.9227695818380741\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol=\"trueLabel\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "aur = evaluator.evaluate(prediction)\n",
    "print (\"AUR = \", aur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac1397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
